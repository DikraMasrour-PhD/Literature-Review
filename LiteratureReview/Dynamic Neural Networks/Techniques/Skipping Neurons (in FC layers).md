---
tags:
  - Technique
  - DNN
  - DynamicWidth
---
### Motivation
Skipping neurons in FC layers scheme relies on the belief that in FC layers different neuron units are responsible for different features, and therefore not all neuron units need to be activated for each input sample.
### Neuron Skipping Schemes
Most of the following techniques are included under the category of Conditional Computation.
#### Auxiliary branches
- [[(2013) Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation|Gradient estimation: Stochastic Neurons for Conditional Computation]]
- [[Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning]]
- [[Conditional Computation in Neural Networks for Faster Models]]
#### Low-rank approximation
[[Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks|Low-Rank Approx]]
